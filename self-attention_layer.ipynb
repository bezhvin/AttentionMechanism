{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we make a more practical usecase of self-attention. In the previous tutorial we presented an intuition of self-attention as finding a new representation for each observation of a smaple by attending to other observations via a weighted averaging. We also briefly refer to the terms *query, key* and *value*. Self-attention layer follows pretty much what we explained before but more expressively by defining learnable parameters for the attention process. In other words, each observation in a sample is transformed depending on its role, if it is about to query other observations, it is tranformed by the query matrix. If it is a key to be queried, it is tranformed by the key matrix, and finally it is tranformed by the value matrix weighted averaging.\n",
    "\n",
    "But before starting with more detailes, let's make a setup for the sake of explaining the self-attention layer intractively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MNIST dataset again. Therefore each sample is an image of 28x28. The observations in the sample could are pixels. However we could still work with patch of pixels instead of pixels individually, which is by the way a common practice. In our case each MNIST image is 28x28 and we can make 16 patches of 7x7 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.data_builder import mnist\n",
    "from src.visualization.plotting import display_patched_image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_data = mnist.MNIST()\n",
    "image_example = mnist_data.train_set.data[0]\n",
    "patches, patch_size = mnist.image_patching(image_example, num_patch=16)\n",
    "display_patched_image(patches, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each subimage or patch above is taken as one observation in the image sample, so basically the image is viewed as a *sequence* of subimages.\n",
    "However, to get the best performance it is quite important to know where each patch belongs to in the original image, in other words the position of each patch matters for recognizing the digits. Therefore, we add an identifier to each patch to contain the positional information. This identifier could be anything as long as it distinctively provides positional information for each patch and it is consistent across all the images. For example, you may want to use numbers from 1 to 16, so that the most left bottom patch is identified by 1 and the most right top one is identified by 16.\n",
    "\n",
    "However, as in many other machine learning approaches, fusing features should be done with care. Adding positional information is not an exception, and therefore in practice this extra feature is added (or concatenated) to the patch representations in more subtle ways than adding integers or one-hot-encoding. This is beyond the scope of this tutorial as we want to stay focused on the self-attention layer. Thus, we just use a positional embedding based on sinusoidal function to identify each image, and then concatenate it to the patch. So in the rest of this tutorial, it is not only the patch that goes to the self-attention layer, but also it's positional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 59])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  src.self_attention.utils import get_sinusoidal_encoding\n",
    "\n",
    "num_patches = 16\n",
    "embedding_dim = 10  # in the case of concatenation this is a hyperparameter \n",
    "positional_embeddings = get_sinusoidal_encoding(num_patches, embedding_dim)\n",
    "\n",
    "input_to_self_attentation_layer = torch.concat((patches.reshape(16,-1), positional_embeddings), axis=1)\n",
    "input_to_self_attentation_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the input image is split into 16 patches of 7x7 and each patch is vectorised and concatenated with 10 variables representing it's position in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the input is ready. lets start with the self-attention layer. This layer basically contains three main linear transformations named *query, key* and *value* matrices, and the output dimension is named attention head size or the embedding size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 59 # 7*7 pixels + 10 positional embedding\n",
    "embed_dim = 10 # attention head size\n",
    "query = torch.nn.Linear(input_dim, embed_dim, bias=False)\n",
    "key = torch.nn.Linear(input_dim, embed_dim, bias=False)\n",
    "value = torch.nn.Linear(input_dim, embed_dim, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we defined three learnable matrixes, which all are supposed to transform the input data. Self-attention can be viewed as weighted linear averaging so that the new representation for each patch is obtained as a linear combination of other pathches. Let's take one patch as an example to elaborate more on the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([59])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_example = input_to_self_attentation_layer[0]\n",
    "patch_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First this patch is transformed via query matrix. This makes it possible to have a more expressive modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_example_query = query(patch_example)\n",
    "patch_example_query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to define the weights the query patch is matched against all the patches, including itself, to measure its similarity via dot product. However, first all the patches to be quiriedare transformed via the key matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_patches = key(input_to_self_attentation_layer)\n",
    "key_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and weights can be then calculated and normalise to be between zero and one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2270e-19, 7.7853e-27, 2.2317e-32, 1.0609e-30, 2.2368e-19, 1.4157e-18,\n",
       "         2.3803e-05, 1.0890e-18, 2.2578e-19, 1.4716e-26, 7.7874e-11, 2.2276e-19,\n",
       "         2.3488e-33, 9.9998e-01, 1.3077e-13, 2.1300e-19]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = patch_example_query @ key_patches.transpose(1,0)\n",
    "weights = F.softmax(weights, dim=-1).unsqueeze(0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the output of the self-attention layer by taking the weghted average of the all other patches given the assigned weights. First, again the patches are first transformed via value matrix and then averaged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[205.5148,  14.7546,  74.4948,  56.7472,  45.2748,  27.4483, -74.9525,\n",
       "          -5.0673,  30.5897, -63.5396]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_patches = value(input_to_self_attentation_layer)\n",
    "attention_output = weights @ value_patches\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the new 10-dimensional representaion of the patch based on the self-attention mechanisms. Noting that the matrixes are learnable and this layer can be trained together with other layers of a neural network given some objectives.\n",
    "\n",
    " Let's make this layer more formally via Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.key = torch.nn.Linear(self.input_dim, self.output_dim, bias=False)\n",
    "        self.query = torch.nn.Linear(self.input_dim, self.output_dim, bias=False)\n",
    "        self.value = torch.nn.Linear(self.input_dim, self.output_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_query = self.query(x)\n",
    "        x_key = self.key(x)\n",
    "        x_value = self.value(x)\n",
    "        weights = F.softmax(x_query @ x_key.transpose(-1,-2), dim=-1)\n",
    "        output = weights @ x_value\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = SelfAttention(input_dim=input_dim, output_dim=embed_dim)\n",
    "attention_layer(input_to_self_attentation_layer.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to use attention mechanisms multiple times to capture different attention directions. In our MNIST example we may want to attend to different pathches to get different information, For example to recognise the digits, we always want to attend to patches in center of the image, but to distinguish between certain digits, let say 2 and 7, maybe extra attention is required to certain patchs. Having multiple attention mechanism also provides expressivness but at the cost of more parameters.\n",
    "\n",
    "multiple attention is called multi-head attention and in practice it is simply applying multiple attention mechansim on the input and merging them via a linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_head = 5\n",
    "multi_head_outputs = []\n",
    "for _ in range(num_head):\n",
    "    attention_layer = SelfAttention(input_dim=input_dim, output_dim=embed_dim)\n",
    "    multi_head_outputs.append(attention_layer(input_to_self_attentation_layer.unsqueeze(0)))\n",
    "multi_head_output = torch.concat(multi_head_outputs, axis=2)\n",
    "multi_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output has 50 dimension (num_head x embed_size). Now we can get back to the requested embed size, 10, by a linear transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output =torch.nn.Linear(num_head*embed_dim, embed_dim)(multi_head_output)\n",
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reza-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
