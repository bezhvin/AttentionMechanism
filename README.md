# Self-attention mechanism
Attention mechanism is maybe on the most effective steps in the realm of AI and deep learning towards more humean like way of thinking so far. Referring to the great book of [Thinking, Fast and slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow), human thinks either based on system1 or system2: system1 thinking is more about intuition and memorization while system2 steps in for reasoning and when attention is required. Although the attention mechanism proposed and utilised in current machine learning models are not exactly mimicing what human brain does, it imporves the model performance in certain cases dramatically by trying to model the behaviour of one of the main aspect of system2 thinking. Given the importance of this research direction and the impact of the current utilisation of attention mechanism specially in the transformer based models, this tutorial is made to provide an educative tool. 

Here is the content of this tutorial:


* [Implicit attention](https://github.com/bezhvin/AttentionMechanism/blob/main/ImplicitAttention.ipynb): This notebook is a tutorial to underestand the concept of attention in machine learning and how actually every machine learning model needs to "attend" to gain reasonable performance, which is also called implicit attention. It is an interactive tutorial with images from MNIST dataset and fitting simple models trying to quantify and explain implicit attention mechanism in any model.

* Self-attention:
   * Concept and intuition:


